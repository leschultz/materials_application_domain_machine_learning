{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TUmrq6NixjN"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/leschultz/materials_application_domain_machine_learning/blob/main/examples/jupyter/tutorial_1.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9YonbivJF1i"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkbvapstI0dv"
   },
   "outputs": [],
   "source": [
    "!pip install madml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8teOKc3SGib"
   },
   "source": [
    "Import packages for run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbLKBwbMSJ5N"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from madml.models import dissimilarity, calibration, domain, combine\n",
    "from madml.splitters import BootstrappedLeaveClusterOut\n",
    "from madml.assess import nested_cv\n",
    "from madml import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmSXxALOSzWI"
   },
   "source": [
    "## Load data\n",
    "There are a set of datasets available. You can load any of them with the name given by the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkAU4MlJTI0M"
   },
   "outputs": [],
   "source": [
    "datasets.list_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOve0Ix6TWtU"
   },
   "source": [
    "Any of the supported data can be loaded in a standard manner. You are capable of loading your own data instead of any of the supported datasest if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3IAIaEYAS5FO"
   },
   "outputs": [],
   "source": [
    "data = datasets.load('strength')\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "g = data['class_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TN0y0eITtpW"
   },
   "source": [
    "## Build model\n",
    "We define three model types: uncertanty quantification, distance, and regression model. If we want uncertainty quantification, the regression model must be an ensmble model (e.g. random forest, bagged LASSO, et cetera)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIhXifFXUIIk"
   },
   "source": [
    "We start with a distance model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xh0Loz7HUB1z"
   },
   "outputs": [],
   "source": [
    "ds_model = dissimilarity(dis='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Oa1QvL2UTp6"
   },
   "source": [
    "Now we add a polynomial uncertaty quantifiction model. The number of arguments for te argument params defines the degree of the polynomial, and their values are the inital guesses to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAS-I9SHUSak"
   },
   "outputs": [],
   "source": [
    "uq_model = calibration(params=[0.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auaYZY1LUjuR"
   },
   "source": [
    "Now we define the regression model. The regression model must be a gridserach object with a pipeline. The example here does not iterate over folds for hyperparamter optimization, but it can be modified to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OB0Uv4fU6Id"
   },
   "outputs": [],
   "source": [
    "# ML\n",
    "scale = StandardScaler()\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# The grid for grid search\n",
    "grid = {}\n",
    "grid['model__n_estimators'] = [100]\n",
    "\n",
    "# The machine learning pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "                        ('scaler', scale),\n",
    "                        ('model', model),\n",
    "                        ])\n",
    "\n",
    "# The gridsearch model\n",
    "gs_model = GridSearchCV(\n",
    "                        pipe,\n",
    "                        grid,\n",
    "                        cv=((slice(None), slice(None)),),  # No splits\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4dq9zc1VMvp"
   },
   "source": [
    "# Building the splits\n",
    "Here comes the fun part. The performance of a model on a test set depends on many things. We want to guard against using predictions on data that are sampled dissimilarly to the data used for training. First, we build splits where test data are sampled similarly to training data. We give these splits the special name of \"fit\" so that we only use this kind of splitter for our uncertainty quantification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKWSvgJcVP-P"
   },
   "outputs": [],
   "source": [
    "n_repeats = 2  # The number of times to repeat splits\n",
    "splits = [('fit', RepeatedKFold(n_repeats=n_repeats))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhC45B31WLeB"
   },
   "source": [
    "How we need to tell the model what data are dissimilar. We use come pre-clustering and split data accordingly. Here, we do 2 and 3 cluster and use agglomerative clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQmagWiqWB75"
   },
   "outputs": [],
   "source": [
    "for i in [2, 3]:\n",
    "\n",
    "    # Cluster Splits\n",
    "    top_split = BootstrappedLeaveClusterOut(\n",
    "                                            AgglomerativeClustering,\n",
    "                                            n_repeats=n_repeats,\n",
    "                                            n_clusters=i\n",
    "                                            )\n",
    "\n",
    "    splits.append(('agglo_{}'.format(i), top_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCL2WngJWphB"
   },
   "source": [
    "# Fitting and Assessing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit a single model without assessment, which is faster because of no nested cross validation. However, overfitting may occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oXHWVwfXOK3"
   },
   "outputs": [],
   "source": [
    "model = combine(gs_model, ds_model, uq_model, splits)\n",
    "model.fit(X, y, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assess the model through neseted cross validation and then fit a final model on all data. The assessment of the model is saved in a directory of the user's choice. The model created here should be the one used for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = nested_cv(model, X, y, splitters=splits)\n",
    "df, df_bin, fit_model = cv.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twibWAl5Xcrt"
   },
   "source": [
    "# Example of Model Use\n",
    "Our assessment returns a model. We can also use dill to load the saved model. Here, we predict on the features used to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTIBKQhJXgT6"
   },
   "outputs": [],
   "source": [
    "df = fit_model.predict(X)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the predefined thresholds for domain are insufficient. We can include a manual threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fit_model.predict(X, 0.5)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
